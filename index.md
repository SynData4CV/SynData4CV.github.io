---
layout: default
title: Synthetic Data for Computer Vision - CVPR 2025
buttons:
  Overview: './index.html'
  Invited Speakers: './index.html#invited-speakers'
  Schedule: './index.html#schedule'
  Call for Papers: './index.html#call-for-papers'
  Important Dates: './index.html#important-workshop-dates'
  Related Workshops: './index.html#related-workshops'
  Organizers: './index.html#organizers'
description:
  - CVPR 2025 Workshop
  - June, 2025
  - Seattle, United States
---

<style> 
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 75%;
} </style>

# Overview
<div style="text-align: left; max-width: 800px; margin: auto;">
The workshop aims to explore the use of synthetic data in training and evaluating computer vision models, as well as in other related domains. During the last decade, advancements in computer vision were catalyzed by the release of painstakingly curated human-labeled datasets. Recently, people have increasingly resorted to synthetic data as an alternative to laborintensive human-labeled datasets for its scalability, customizability, and costeffectiveness. Synthetic data offers the potential to generate large volumes of diverse and high-quality vision data, tailored to specific scenarios and edge cases that are hard to capture in real-world data. However, challenges such as the domain gap between synthetic and real-world data, potential biases in synthetic generation, and ensuring the generalizability of models trained on synthetic data remain. We hope the workshop can provide a forum to discuss and encourage further exploration in these areas.
</div>

# Invited Speakers 
* Stay tuned for updates!


# Schedule
* Stay tuned for updates!


# Poster Session
* Stay tuned for updates!

# Awards
* Stay tuned for updates!






  


<!-- ## Advising committee -->

<!-- <div style="display: flex">
 <div style="width:22.5%">
    <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">
    <img alt="name_16" src="pics/placeholder.jpg"  height="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">[Name]</a><br>
    [Institution]
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">
    <img alt="name_16" src="pics/placeholder.jpg"  height="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">[Name]</a><br>
    [Institution]
  </div>
</div> -->



<!-- ## Program Committee -->
<!-- 
| --- | --- |
|  |  | -->

<!-- ## Student Organizers -->
<!-- 
| --- | --- |
|  |  |
 -->


## Call for papers
* Stay tuned for updates!
<!-- Please refer to the **[call for papers](./call-for-papers.html)** page for more details. -->

<!-- 
<div style="text-align: center">
<u><g8>Challenge</g8></u>
</div>
 -->

<!-- ## Challenge overview -->
<!-- 
<div style="text-align: justify">


Towards building a community of accessibility research in computer vision conferences, we introduce a computer vision challenge with synthetic and real-world benchmarks. The challenge (based on our ICCV’21 paper, <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>) will be used to benchmark various computer vision tasks when comparing new and established methods for fine-grained perception of tasks relevant to people with disabilities. The challenge is designed in the spirit of various other vision challenges that help advance the state-of-the-art of computer vision for autonomous systems, e.g., in robust vision (CVPR’21), human action recognition trajectory forecasting (CVPR’21), etc. E
 </div>
<div class = "center">
    <img alt="fig1" src="pics/fig1.svg" >
    <p>Fig. 1: An interactive simulation environment will be used as part of the workshop challenge for training machine perception and learning models in the context of accessibility (taken from <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>).</p>
<br> 
<div class = "center">
    <img alt="fig2" src="pics/fig2.svg" >
    <p>An example from the instance segmentation challenge for perceiving people with mobility aids.</p>
</div>
<br> 
</div>
<br>-->


<!-- ## Challenge Organization

<div style="display: flex">
  <div style="width:22.5%">
    <a href="mailto:sgzk@bu.edu">
    <img alt="Zhongkai Shangguan" src="pics/zhongkai_shangguan.png"   style =  "border-radius: 50%; object-fit: cover; width = 100% ">
    </a><br>
  <a href="mailto:sgzk@bu.edu">Zhongkai Shangguan</a><br>
    Boston University
  </div>
  
    <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="mailto:zhangjim@bu.edu">
    <img alt="Jimuyang Zhang" src="pics/jimuyang_zhang.jpg"  style =  "border-radius: 50%; object-fit: cover; width = 100% ">
    </a><br>
    <a href="mailto:zhangjim@bu.edu">Jimuyang Zhang</a><br>
    Boston University
  </div>

</div> -->

  




<!-- ## Challenge

<div style="text-align: justify">
  
  <strong>As an updated challenge for 2023, we release the following:</strong>
  
  <ol>
  <li>Training, validation, and testing data, which can be found in <a href="https://drive.google.com/drive/folders/12e-Qom2qQWF7brBu36sIQZWfj8kTBtj-?usp=share_link">this link</a></li>
    <li>An evaluation server <a href="https://eval.ai/web/challenges/challenge-page/1998/overview">for instance segmentation</a> and <a href="https://eval.ai/web/challenges/challenge-page/2001/overview">for pose estimation.</a></li>
  </ol>
  
  More info on data and submission can be found in the eval.ai links above. Note that the data this year includes both instance segmentation and pose estimation challenge. Moreover, we provide access to temporal history and LiDAR data for each image.
  
  <br>
  The challenge builds on our prior workshop's synthetic instance segmentation benchmark with mobility aids (see Zhang et al., X-World: Accessibility, Vision, and Autonomy Meet, ICCV 2021 <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>). The benchmark contains challenging accessibility-related person and object categories, such as `cane' and `wheelchair.' We aim to use the challenge to uncover research opportunities and spark the interest of computer vision and AI researchers working on more robust visual reasoning models for accessibility. 
  
<div class = "center">
    <img alt="fig2" src="pics/i1.jpg" >
    <p>An example from the instance segmentation challenge for perceiving people with mobility aids.</p>
</div>
  <div class = "center">
    <img alt="fig2" src="pics/pose_xworld.png" >
    <p>An example from the pose challenge added in 2023.</p>
</div>
  <br>
  The team with the top performing submission will be invited to give short talks during the workshop and will receive a financial award of <b>$500</b> and an <a href="https://store.opencv.ai/products/oak-d">OAK—D camera</a> (We thank the National Science Foundation, US Department of Transportation's Inclusive Design Challenge and Intel for their support for these awards) 
  <br><br>
</div> -->

 
  
  
  
# Call for Papers
We invite papers on **the use of synthetic data for training and
evaluating computer vision models.** We welcome submissions along two
tracks:

- **Full papers:** Up to 8 pages, not including references/appendix.

- **Short papers:** Up to 4 pages, not including references/appendix.

Accepted papers will be allocated a poster presentation and displayed on
the workshop website. In addition, we will offer a Best Long Paper
award, Best Paper Runner-up award, and Best Short Paper with oral
presentation.

### Topics

Potential topics include, but are not limited to:

- **Effectiveness:** What is the most effective way to generate and
    leverage synthetic data? How \"realistic\" does synthetic data need
    to be?

- **Efficiency and scalability:** Can we make synthetic data
    generation more efficient and scalable without sacrificing quality?

- **Benchmark and evaluation:** What benchmark and evaluation methods
    are needed to assess the efficacy of synthetic data for computer
    vision?

- **Risks and ethical considerations:** What ethical questions and
    risks are associated with synthetic data (*e.g.* bias
    amplification), and how can we address them?

- **Applications:** In addition to existing attempts on leveraging
    synthetic data for training visual recognition and vision-language
    models, what are other tasks in computer vision or other related
    fields (*e.g.*, robotics, NLP) that could benefit from synthetic
    data?

- **Other open problems:** How do we decide which type of data to use,
    synthetic or real-world data? What is the optimal way to combine
    both if both are available? How much real-world data do we need (in
    the long run)?

### Submission Instructions

<!-- Submissions should be anonymized and formatted using the [CVPR 2025
template](http://google.com) and uploaded as a single PDF. 
Note that our workshop is non-archival.\
\
**Submission link:** [OpenReview Link](https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/SynData4CV) -->


# Important workshop dates
<!-- - Updated challenge release: <strong>3/18/2023</strong>
- Workshop abstract submission deadline: <strong>6/11/2023</strong> (11:59PM PST, please submit extended abstracts via email to mobility@bu.edu) 
- Challenge submission deadline: <strong>6/11/2023</strong> 
- Abstract notification: <strong>6/13/2023</strong>
- Challenge winner announcement: <strong>6/18/2023</strong>
- TBD -->
<!-- - Deadline for submission: ~~<strong>March 15th, 11:59 PM Pacific Time</strong>~~ <strong>March 30th, 11:59 PM Pacific Time</strong>
- Notification of acceptance: <strong>April 9th, 11:59 PM Pacific Time</strong>
- Camera Ready submission deadline: <strong>April 24th, 11:59 PM Pacific Time</strong>
- Workshop date: <strong>June 18th, 2025 (Full day)</strong> -->


<!-- ### Join our **[mailing list](https://staging-temp-site.github.io/staging-temp-site.gitub.io/)** for updates. -->

<!-- ## Videos -->

<!-- <div style=" float: center;">
    <div align="center" style="width:45%; float: left;">
      <h4><u>OpenGuide</u> </h4>
        <iframe src="https://www.youtube.com/embed/mGq9sL1spzc" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);height: 30vh" allowfullscreen></iframe>
    </div>
    <div style="width:5%; float: left;">
        <p></p>
    </div>
    
    <!--div align="center"  style="width:45%; float: left;">
      <h4 ><u>X-World</u> </h4>
      
        <iframe src="https://www.youtube.com/embed/z_YwWIZWg58" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px); height: 30vh" allowfullscreen></iframe>
      
    </div>
  </div--> 
  
# Related Workshops
- <a href="https://syntml-cvpr2022-workshop.github.io/">Machine Learning with Synthetic Data @ CVPR 2022</a>
- <a href="https://sites.google.com/view/sdas2023/">Synthetic Data for Autonomous Systems @ CVPR 2023</a>
- <a href="https://www.syntheticdata4ml.vanderschaar-lab.com/">Synthetic Data Generation with Generative AI @ NeurIPS 2023</a>

  

# Organizers
<div style="display: flex; flex-wrap: wrap; justify-content: space-around;">
  
  <!-- Organizer 1 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://jieyuz2.github.io/">
      <img alt="Jieyu Zhang" src="pics/organizers/jieyuzhang.png" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://jieyuz2.github.io/">Jieyu Zhang</a><br>
    University of Washington
  </div>

  <!-- Organizer 2 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://chengyuhsieh.github.io/">
      <img alt="Cheng-Yu Hsieh" src="pics/organizers/chengyuhsieh.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://chengyuhsieh.github.io/">Cheng-Yu Hsieh</a><br>
    University of Washington
  </div>

  <!-- Organizer 3 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://zixianma.github.io/">
      <img alt="Zixian Ma" src="pics/organizers/zixianma.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://zixianma.github.io/">Zixian Ma</a><br>
    University of Washington
  </div>

  <!-- Organizer 4 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://ssundaram21.github.io/">
      <img alt="Shobhita Sundaram" src="pics/organizers/ssundaram.png" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://ssundaram21.github.io/">Shobhita Sundaram</a><br>
    Massachusetts Institute of Technology
  </div>

  <!-- Organizer 5 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://weikaih2004.github.io/">
      <img alt="Weikai Huang" src="pics/organizers/weikaihuang.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://weikaih2004.github.io/">Weikai Huang</a><br>
    University of Washington
  </div>

  <!-- Organizer 6 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://people.csail.mit.edu/weichium/">
      <img alt="Wei-Chiu Ma" src="pics/organizers/weichiuma.png" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://people.csail.mit.edu/weichium/">Wei-Chiu Ma</a><br>
    Cornell University
  </div>



  <!-- Organizer 7 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://web.mit.edu/phillipi/">
      <img alt="Phillip Isola" src="pics/organizers/phillipisola.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://web.mit.edu/phillipi/">Phillip Isola</a><br>
    Massachusetts Institute of Technology
  </div>

  <!-- Organizer 8 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://ranjaykrishna.com/index.html">
      <img alt="Ranjay Krishna" src="pics/organizers/ranjaykrishna.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;">
    </a><br>
    <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a><br>
    University of Washington
  </div>
</div>



<!-- ### Access CVPR 2024 Workshop
If you are looking for details about the previous workshop, click the button below:

[Go to CVPR 2024 Workshop](cvpr2024.html)
</div> -->